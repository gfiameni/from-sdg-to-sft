{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ac2b61-ccdf-41f6-872e-d7ab3705ae8b",
   "metadata": {},
   "source": [
    "# From SDF to SFT with LLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceab580-e8f6-4d3c-b56a-6275a087b732",
   "metadata": {},
   "source": [
    "This playbook demonstrates how to fine tune a model on a synthetic generated data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f5a8d-38b4-4b80-9fd3-cf6afb05adf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Access to at lest 4 GPUs\n",
    "- NGC Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc9774-74b8-4a6c-bbb2-4d019f976bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section one: Synthetic Data Generation\n",
    "\n",
    "This section of this tutorial aims to demonstrate a basic loop with two stages as follows. These stages are repeated until the desired dataset size is achieved:\n",
    "\n",
    "\n",
    "Data processing: perform operations such as HTML tag cleaning, quality-based filtering and semantic deduplication on the records. Synthetic data generation: query a synthetic data generation model (such as LLaMa 3.1 405B Instruct, or Nemotron-4 340B Instruct) to produce synthetic variants of existing records. Each synthetic record is then fed to a reward model (such as Nemotron-4 340B Reward), and assigned a quality score. All records are then fed to the data processing stage for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44bd9031-2a2d-4541-9b26-0aba14e2dada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d96b60bf-94fd-43c5-b28a-bc2131101386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEMO_DIR = os.path.join(\"/opt/NeMo\")\n",
    "NEMO_CURATOR_DIR = os.path.join(\"/opt/NeMo-Curator\")\n",
    "HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a4b1189-e9b4-4fe3-aa1a-c61d51e3f92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOUR_WORKING_DIR = os.path.join(os.path.expanduser('~'), \"exp1\")\n",
    "os.makedirs(YOUR_WORKING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4a2451-943d-41d0-b4f0-8417ecf733ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x 14 root root 4096 Aug  2 22:47 /opt/NeMo\n",
      "drwxr-xr-x 11 root root 4096 Aug  2 22:41 /opt/NeMo-Curator\n",
      "drwxr-xr-x  2 root root 4096 Oct  9 09:42 /root/exp1\n"
     ]
    }
   ],
   "source": [
    "!ls -ld {NEMO_DIR} {NEMO_CURATOR_DIR} {YOUR_WORKING_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1194404a-ff93-4c07-ae6e-a406c4ff62c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nemo-curator'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0056733c-a81d-4801-a1da-b27f493a6482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  /root/exp1/data/raw/downloads\n",
      "Downloading Law QA dataset from 'https://huggingface.co/datasets/ymoslem/Law-StackExchange/resolve/main/law-stackexchange-questions-answers.json'...\n",
      "Running the initial curation pipeline on '/root/exp1/data/raw/splits/law-qa-train.jsonl'...\n",
      "Reading 1 files\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "tokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 4.29MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 25.3MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 24.8MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 112/112 [00:00<00:00, 1.26MB/s]\n",
      "config.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 8.61MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "model.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 231MB/s]\n",
      "Fitting memory estimate curve for model: sentence-transformers/all-MiniLM-L6-v2\n",
      " 38%|████████████████▉                            | 3/8 [00:24<00:39,  7.98s/it]2024-10-09 09:44:26,844 - distributed.utils_perf - WARNING - full garbage collections took 39% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:27,474 - distributed.utils_perf - WARNING - full garbage collections took 41% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:27,894 - distributed.utils_perf - WARNING - full garbage collections took 43% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:28,528 - distributed.utils_perf - WARNING - full garbage collections took 44% CPU time recently (threshold: 10%)\n",
      " 50%|██████████████████████▌                      | 4/8 [00:31<00:30,  7.56s/it]2024-10-09 09:44:29,649 - distributed.utils_perf - WARNING - full garbage collections took 45% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:30,819 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:31,944 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:32,542 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:33,110 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:33,467 - distributed.utils_perf - WARNING - full garbage collections took 48% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:33,941 - distributed.utils_perf - WARNING - full garbage collections took 49% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:34,274 - distributed.utils_perf - WARNING - full garbage collections took 51% CPU time recently (threshold: 10%)\n",
      " 62%|████████████████████████████▏                | 5/8 [00:37<00:20,  6.96s/it]2024-10-09 09:44:34,780 - distributed.utils_perf - WARNING - full garbage collections took 53% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:35,924 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:36,543 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:37,152 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:37,517 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:37,935 - distributed.utils_perf - WARNING - full garbage collections took 56% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:38,284 - distributed.utils_perf - WARNING - full garbage collections took 57% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:38,788 - distributed.utils_perf - WARNING - full garbage collections took 60% CPU time recently (threshold: 10%)\n",
      " 75%|█████████████████████████████████▊           | 6/8 [00:41<00:12,  6.08s/it]2024-10-09 09:44:39,155 - distributed.utils_perf - WARNING - full garbage collections took 60% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:39,593 - distributed.utils_perf - WARNING - full garbage collections took 63% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:40,121 - distributed.utils_perf - WARNING - full garbage collections took 64% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:40,480 - distributed.utils_perf - WARNING - full garbage collections took 64% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:40,843 - distributed.utils_perf - WARNING - full garbage collections took 64% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:41,215 - distributed.utils_perf - WARNING - full garbage collections took 64% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:41,629 - distributed.utils_perf - WARNING - full garbage collections took 65% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:42,014 - distributed.utils_perf - WARNING - full garbage collections took 70% CPU time recently (threshold: 10%)\n",
      " 88%|███████████████████████████████████████▍     | 7/8 [00:44<00:05,  5.15s/it]2024-10-09 09:44:42,391 - distributed.utils_perf - WARNING - full garbage collections took 74% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:42,966 - distributed.utils_perf - WARNING - full garbage collections took 74% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:43,332 - distributed.utils_perf - WARNING - full garbage collections took 75% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:43,774 - distributed.utils_perf - WARNING - full garbage collections took 77% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:44,119 - distributed.utils_perf - WARNING - full garbage collections took 77% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:44,492 - distributed.utils_perf - WARNING - full garbage collections took 78% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:44,820 - distributed.utils_perf - WARNING - full garbage collections took 81% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:45,282 - distributed.utils_perf - WARNING - full garbage collections took 81% CPU time recently (threshold: 10%)\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:48<00:00,  6.00s/it]\n",
      "2024-10-09 09:44:45,599 - distributed.utils_perf - WARNING - full garbage collections took 82% CPU time recently (threshold: 10%)\n",
      "2024-10-09 09:44:45,603 | 7086167 | Rank 0 | Clustering output directory _temp/semdedup_cache/clustering_results already exists and will be overwritten\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU: 0, Part: 0:   0%|                                | 0/12244 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU: 0, Part: 0: 100%|██████████████████| 12244/12244 [00:08<00:00, 1494.57it/s]\n",
      "Writing to disk complete for 1 partitions\n",
      "2024-10-09 09:45:41,368 | 7086167 | Rank 0 | KMeans starting fit\n",
      "2024-10-09 09:45:41,368 | 7086167 | Rank 0 | KMeans starting fit\n",
      "9403392\n",
      "2024-10-09 09:45:44,084 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:45:44,084 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:45:44,085 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:45:44,085 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:45:44,361 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:45:44,361 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:45:44,362 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:45:44,362 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:45:46,239 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:45:46,239 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:45:46,240 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:45:46,240 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:45:46,241 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:45:46,241 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:45:50,670 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:45:50,670 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:45:50,673 | 7086167 | Rank 0 | Time for ranking: 0.07 mins\n",
      "2024-10-09 09:45:50,673 | 7086167 | Rank 0 | Time for ranking: 0.07 mins\n",
      "2024-10-09 09:45:50,673 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:45:50,673 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:45:55,310 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:45:55,310 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:45:56,600 | 7086167 | Rank 0 | DONE saving 24488 out of 24488. Removed: 0. Epsilon: 0.0100\n",
      "2024-10-09 09:45:56,600 | 7086167 | Rank 0 | DONE saving 24488 out of 24488. Removed: 0. Epsilon: 0.0100\n",
      "Reading 20 files\n",
      "After the initial curation, the dataset has 12244 records (originally 19474).\n",
      "--------------------------------------------------------------------------------\n",
      "Running synthetic data generation -- round 1 (out of 1)...\n",
      "Synthesizing 12 rows:   0%|                               | 0/1 [00:00<?, ?it/s]\n",
      "---- Rows 0 to 12:   0%|                                 | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "---- Rows 0 to 12:   8%|██                       | 1/12 [00:09<01:44,  9.51s/it]\u001b[A\n",
      "---- Rows 0 to 12:  17%|████▏                    | 2/12 [00:10<00:42,  4.24s/it]\u001b[A\n",
      "---- Rows 0 to 12:  25%|██████▎                  | 3/12 [00:10<00:21,  2.43s/it]\u001b[A\n",
      "---- Rows 0 to 12:  33%|████████▎                | 4/12 [00:10<00:12,  1.52s/it]\u001b[A\n",
      "---- Rows 0 to 12:  42%|██████████▍              | 5/12 [00:10<00:07,  1.10s/it]\u001b[A\n",
      "---- Rows 0 to 12:  50%|████████████▌            | 6/12 [00:10<00:04,  1.32it/s]\u001b[A\n",
      "---- Rows 0 to 12:  58%|██████████████▌          | 7/12 [00:12<00:04,  1.07it/s]\u001b[A\n",
      "---- Rows 0 to 12:  67%|████████████████▋        | 8/12 [00:13<00:03,  1.11it/s]\u001b[A\n",
      "---- Rows 0 to 12:  75%|██████████████████▊      | 9/12 [00:13<00:02,  1.23it/s]\u001b[A\n",
      "---- Rows 0 to 12:  83%|████████████████████    | 10/12 [00:13<00:01,  1.61it/s]\u001b[A\n",
      "---- Rows 0 to 12:  92%|██████████████████████  | 11/12 [00:14<00:00,  1.98it/s]\u001b[A\n",
      "---- Rows 0 to 12: 100%|████████████████████████| 12/12 [00:14<00:00,  1.19s/it]\u001b[A\n",
      "Synthesizing 12 rows: 100%|███████████████████████| 1/1 [00:14<00:00, 14.23s/it]\n",
      "Reading 2 files\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-10-09 09:46:56,255 | 7086167 | Rank 0 | Clustering output directory _temp/semdedup_cache/clustering_results already exists and will be overwritten\n",
      "2024-10-09 09:46:56,255 | 7086167 | Rank 0 | Clustering output directory _temp/semdedup_cache/clustering_results already exists and will be overwritten\n",
      "2024-10-09 09:46:56,255 | 7086167 | Rank 0 | Clustering output directory _temp/semdedup_cache/clustering_results already exists and will be overwritten\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU: 0, Part: 0: 100%|████████████████████████████| 5/5 [00:00<00:00, 15.82it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU: 0, Part: 1:   0%|                                | 0/12244 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU: 0, Part: 1: 100%|██████████████████| 12244/12244 [00:07<00:00, 1637.36it/s]\n",
      "Writing to disk complete for 2 partitions\n",
      "2024-10-09 09:47:17,721 | 7086167 | Rank 0 | KMeans starting fit\n",
      "2024-10-09 09:47:17,721 | 7086167 | Rank 0 | KMeans starting fit\n",
      "2024-10-09 09:47:17,721 | 7086167 | Rank 0 | KMeans starting fit\n",
      "2024-10-09 09:47:17,721 | 7086167 | Rank 0 | KMeans starting fit\n",
      "4703616\n",
      "2024-10-09 09:47:19,368 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:47:19,368 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:47:19,368 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:47:19,368 | 7086167 | Rank 0 | KMeans fit complete\n",
      "2024-10-09 09:47:19,369 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:47:19,369 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:47:19,369 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:47:19,369 | 7086167 | Rank 0 | Computing nearest centroids + distance to centers using kmeans.predict\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Saving centroids complete\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:47:19,412 | 7086167 | Rank 0 | Output directory _temp/semdedup_cache/clustering_results/embs_by_nearest_center already exists and will be overwritten\n",
      "2024-10-09 09:47:20,549 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:47:20,549 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:47:20,549 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:47:20,549 | 7086167 | Rank 0 | Saved embeddings by nearest center to _temp/semdedup_cache/clustering_results/embs_by_nearest_center\n",
      "2024-10-09 09:47:20,550 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:47:20,550 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:47:20,550 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:47:20,550 | 7086167 | Rank 0 | Ranking...\n",
      "2024-10-09 09:47:20,551 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:47:20,551 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:47:20,551 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:47:20,551 | 7086167 | Rank 0 | Removing existing sorted cluster directory: _temp/semdedup_cache/clustering_results/sorted\n",
      "2024-10-09 09:47:21,930 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:47:21,930 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:47:21,930 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:47:21,930 | 7086167 | Rank 0 | Completed 20 clusters. Missing 0 clusters.\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | Time for ranking: 0.02 mins\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | Time for ranking: 0.02 mins\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | Time for ranking: 0.02 mins\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | Time for ranking: 0.02 mins\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:47:21,932 | 7086167 | Rank 0 | DONE!\n",
      "2024-10-09 09:47:22,179 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:47:22,179 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:47:22,179 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:47:22,179 | 7086167 | Rank 0 | Removing existing directory _temp/semdedup_cache/clustering_results/semdedup_pruning_tables\n",
      "2024-10-09 09:47:23,362 | 7086167 | Rank 0 | DONE saving 12249 out of 12249. Removed: 0. Epsilon: 0.0100\n",
      "2024-10-09 09:47:23,362 | 7086167 | Rank 0 | DONE saving 12249 out of 12249. Removed: 0. Epsilon: 0.0100\n",
      "2024-10-09 09:47:23,362 | 7086167 | Rank 0 | DONE saving 12249 out of 12249. Removed: 0. Epsilon: 0.0100\n",
      "2024-10-09 09:47:23,362 | 7086167 | Rank 0 | DONE saving 12249 out of 12249. Removed: 0. Epsilon: 0.0100\n",
      "Reading 20 files\n",
      "After round 1, the dataset has 12249 records (originally 12256).\n",
      "Curated files are saved in '/root/exp1/data/curated/final'.\n"
     ]
    }
   ],
   "source": [
    "!python peft-curation-with-sdg/main.py \\\n",
    "    --api-key \"\" \\\n",
    "    --device gpu \\\n",
    "    --synth-gen-rounds 1 --synth-gen-ratio 0.001 --synth-gen-model \"nvidia/nemotron-4-340b-instruct\" \\\n",
    "    --working-dir {YOUR_WORKING_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a03f1e-efa7-4ca2-bbb3-73f2c9fa0467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x 2 root root 4096 Oct  9 09:47 final\n",
      "drwxr-xr-x 2 root root 4096 Oct  9 09:46 round-1\n"
     ]
    }
   ],
   "source": [
    "!ls -l {YOUR_WORKING_DIR}/data/curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f71af2d-296b-4512-8e3c-c4d075478166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "law-qa-test.jsonl  law-qa-train.jsonl  law-qa-val.jsonl\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(YOUR_WORKING_DIR, \"data/curated/final\")\n",
    "!ls {DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535932fd-5169-4731-8d6a-44e16363122f",
   "metadata": {},
   "source": [
    "You should see the law-qa-{train/val/test}.jsonl splits resulting from following the abovementioned SDG tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8779e0bc-9397-4283-b270-8f7ad23d07a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DS = os.path.join(DATA_DIR, \"law-qa-train.jsonl\")\n",
    "VAL_DS = os.path.join(DATA_DIR, \"law-qa-val.jsonl\")\n",
    "TEST_DS = os.path.join(DATA_DIR, \"law-qa-test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475af12d-7e42-44a0-9902-cf5057b1abbf",
   "metadata": {},
   "source": [
    "2. **Get the model**: Download the `Meta Llama 3.1 8B Instruct .nemo` model and mount the corresponding folder to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b67e8e34-784e-4012-8cd6-63022bd16a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {YOUR_WORKING_DIR}/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd4ad89-9feb-4b62-86ae-8c760c73897e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-09 09:55:26--  https://api.ngc.nvidia.com/v2/models/nvidia/nemo/llama-3_1-8b-instruct-nemo/versions/1.0/zip\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.149.87.155, 34.223.159.105\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.149.87.155|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://files.ngc.nvidia.com/org/nvidia/team/nemo/models/llama-3_1-8b-instruct-nemo/versions/1.0/files.zip?versionId=rh8etjB3R7KsBSm8C3GnCV6pRPpjAAcX&Expires=1728554127&Signature=szoLx3aTZiq6G4aWF63ebhyKdFvzD4iluUFB091CMV~6jRC27BgZST1sl0ffUFy5NcdVbpdaBnqbRLe4AfvTKGSFcJgzt3CDHCQn3QRvNK5KlUtaAHxJGUaGfgNZRzW8MfDYbPds57FYtVQfSJXRqPWzFcZ51o~~JMnk7Y2X5NkSp~8tXnet4IjI-Sa0u0j2-rY-Ac99rNIkP2djKP7jEQrGgCrmcIYnaJ5wxB7-m5Urhe4hTWKYCUzD88LceNAtOxCKHX~hJyXYrjlAh7EqpfWcrtn0QvK1NEJy2XzKO-sRHNR1dLYjkM7xXQ94zeuk19QWjQI4b3gcc9ONkzkPjg__&Key-Pair-Id=KCX06E8E9L60W [following]\n",
      "--2024-10-09 09:55:27--  https://files.ngc.nvidia.com/org/nvidia/team/nemo/models/llama-3_1-8b-instruct-nemo/versions/1.0/files.zip?versionId=rh8etjB3R7KsBSm8C3GnCV6pRPpjAAcX&Expires=1728554127&Signature=szoLx3aTZiq6G4aWF63ebhyKdFvzD4iluUFB091CMV~6jRC27BgZST1sl0ffUFy5NcdVbpdaBnqbRLe4AfvTKGSFcJgzt3CDHCQn3QRvNK5KlUtaAHxJGUaGfgNZRzW8MfDYbPds57FYtVQfSJXRqPWzFcZ51o~~JMnk7Y2X5NkSp~8tXnet4IjI-Sa0u0j2-rY-Ac99rNIkP2djKP7jEQrGgCrmcIYnaJ5wxB7-m5Urhe4hTWKYCUzD88LceNAtOxCKHX~hJyXYrjlAh7EqpfWcrtn0QvK1NEJy2XzKO-sRHNR1dLYjkM7xXQ94zeuk19QWjQI4b3gcc9ONkzkPjg__&Key-Pair-Id=KCX06E8E9L60W\n",
      "Resolving files.ngc.nvidia.com (files.ngc.nvidia.com)... 3.167.192.81, 3.167.192.83, 3.167.192.49, ...\n",
      "Connecting to files.ngc.nvidia.com (files.ngc.nvidia.com)|3.167.192.81|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12678110228 (12G) [binary/octet-stream]\n",
      "Saving to: ‘/root/exp1/model/llama-3_1-8b-instruct-nemo_1.0.zip’\n",
      "\n",
      "/root/exp1/model/ll 100%[===================>]  11.81G  68.7MB/s    in 4m 14s  \n",
      "\n",
      "2024-10-09 09:59:42 (47.5 MB/s) - ‘/root/exp1/model/llama-3_1-8b-instruct-nemo_1.0.zip’ saved [12678110228/12678110228]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/nemo/llama-3_1-8b-instruct-nemo/versions/1.0/zip -O {YOUR_WORKING_DIR}/model/llama-3_1-8b-instruct-nemo_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "255a6db7-225e-44e7-8b84-ee09b5c901ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /root/exp1/model/llama-3_1-8b-instruct-nemo_1.0.zip\n",
      "  inflating: /root/exp1/model/llama3_1_8b_instruct.nemo  \n"
     ]
    }
   ],
   "source": [
    "!unzip {YOUR_WORKING_DIR}/model/llama-3_1-8b-instruct-nemo_1.0.zip -d {YOUR_WORKING_DIR}/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16dad1db-3842-421a-ad9c-8704302e7764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-3_1-8b-instruct-nemo_1.0.zip  llama3_1_8b_instruct.nemo\n"
     ]
    }
   ],
   "source": [
    "!ls {YOUR_WORKING_DIR}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f5580-a171-4880-b914-92020fe91b8a",
   "metadata": {},
   "source": [
    "### Set the Hugging Face Access Token: You can obtain this from your [Hugging Face account](https://huggingface.co/docs/hub/en/security-tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60b53847-fc69-4216-bc63-f485919ecf01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5cdd4-4ceb-4b6c-aa33-0d91afa01cce",
   "metadata": {},
   "source": [
    "---\n",
    "##  Section 2: Data Curation\n",
    "\n",
    "This notebook is structured into four steps:\n",
    "1. Prepare the dataset\n",
    "2. Run the PEFT finetuning script\n",
    "3. Inference with NeMo Framework\n",
    "4. Check the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535885f-1932-438e-94db-ab63b8cc3934",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the dataset\n",
    "\n",
    "This dataset has already undergone several filtering and processing operations, and it can be used to train the model for various different tasks - question title generation (summarization), law domain question answering, and question tag generation (multi-label classification).\n",
    "\n",
    "Take a look at a single row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5283230b-a4b5-42b4-a8ac-04b954828152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\":\"To find out who owns a property in Australia, you can contact your local council or the Land Titles Office. You cannot take action on the property without the owner's consent. Reach out to the owner and request permission. If they refuse and the property poses a fire hazard, consult a solicitor. The solicitor can send a notice to the owner, making them aware of potential liability for damages. This may encourage the owner to take action or grant you permission to address the issue.\",\"answer_score\":0,\"filename\":\"law-qa-train-synth-round-1.jsonl\",\"id\":\"law-stackexchange-qa-5126-synth-0\",\"question\":\"I'm concerned about a property near mine, covered in tall grass and Gorse Bushes, which could exacerbate a fire during a fire ban. I'd like to clear the land for fire defense but am unsure of its ownership. I'm seeking legal advice on how to address this potential hazard, as neither the local council nor any private owner seems to be taking action.\",\"question_score\":0,\"tags\":\"australia\",\"title\":\"Which two offices can you contact in Australia to find out who owns a property?\"}\n"
     ]
    }
   ],
   "source": [
    "# TRAIN, VAL and TEST splits all follow the same structure\n",
    "!head -n1 {TRAIN_DS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44568f3e-7c32-4061-9f75-0553a1f0f151",
   "metadata": {},
   "source": [
    "You will see several fields in the `.jsonl`, including `title`, `question`, `answer`, and other associated metadata.\n",
    "\n",
    "For this tutorial, our input will be the `answer` field, and output will be it's `title`. \n",
    "\n",
    "The following cell does two things -\n",
    "* Adds a template - a prompt instruction (which is optional), and format `{PROMPT} \\nQUESTION: {data[\"question\"]} \\nTITLE: `.\n",
    "* Saves the data splits into the same location, also appending a `_preprocessed` marker to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2d94739-8a20-4f58-af23-15e79e729a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /root/exp1/data/curated/final/law-qa-train.jsonl and created /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl\n",
      "Processed /root/exp1/data/curated/final/law-qa-val.jsonl and created /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl\n",
      "Processed /root/exp1/data/curated/final/law-qa-test.jsonl and created /root/exp1/data/curated/final/law-qa-test_preprocessed.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Add a prompt instruction.\n",
    "PROMPT='''Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more.'''\n",
    "\n",
    "# Creates a preprocessed version of the data files\n",
    "for input_file in [TRAIN_DS, VAL_DS, TEST_DS]:\n",
    "    output_file = input_file.rsplit('.', 1)[0] + '_preprocessed.jsonl'\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Parse each line as JSON\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Create a new dictionary with only the desired fields, renamed and formatted\n",
    "            new_data = {\n",
    "                \"input\": f'''{PROMPT} \\nQUESTION: {data[\"question\"]} \\nTITLE: ''',\n",
    "                \"output\": data['title']\n",
    "            }\n",
    "\n",
    "            # Write the new data as a JSON line to the output file\n",
    "            json.dump(new_data, outfile)\n",
    "            outfile.write('\\n')  # Add a newline after each JSON object\n",
    "\n",
    "    print(f\"Processed {input_file} and created {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e1ff2-ed53-4a86-a95b-f81775168363",
   "metadata": {},
   "source": [
    "After running the above scripts, you will see  `law-qa-{train/test/val}_preprocessed.jsonl` files appear in the data directory.\n",
    "\n",
    "This is what an example will be formatted like -\n",
    "\n",
    "```json\n",
    "{\"input\": \"Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more. \\nQUESTION: In order to be sued in a particular jurisdiction, say New York, a company must have a minimal business presence in the jurisdiction. What constitutes such a presence? Suppose the company engaged a New York-based Plaintiff, and its representatives signed the contract with the Plaintiff in New York City. Does this satisfy the minimum presence rule? Suppose, instead, the plaintiff and contract signing were in New Jersey, but the company hired a law firm with offices in New York City. Does this qualify? \\nTITLE: \", \n",
    " \"output\": \"What constitutes \\\"doing business in a jurisdiction?\\\"\"}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c4f9ae9-a6f7-489d-ba54-b3d605692988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/exp1/data/curated/final/*idx*': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# clear up any cached mem-map file\n",
    "!rm {DATA_DIR}/*idx*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05210705-303a-4b55-9b9e-37befa86f7da",
   "metadata": {},
   "source": [
    "### Step 2: Run PEFT finetuning script for LoRA\n",
    "\n",
    "NeMo framework includes a high level python script for fine-tuning  [megatron_gpt_finetuning.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py) that can abstract away some of the lower level API calls. Once you have your model downloaded and the dataset ready, LoRA fine-tuning with NeMo is essentially just running this script!\n",
    "\n",
    "For this demonstration, this training run is capped by `max_steps`, and validation is carried out every `val_check_interval` steps. If the validation loss does not improve after a few checks, training is halted to avoid overfitting.\n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your train, test and validation data files as well as path to the .nemo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aa3e91c-5cb5-4d82-b0ad-f2b24fb66831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/exp1/data/curated/final\n"
     ]
    }
   ],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "973cb8ea-d606-4faa-880c-762c47cc20a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen': No such file or directory\n",
      "[2024-10-09 10:08:52,279] torch.distributed.run: [WARNING] \n",
      "[2024-10-09 10:08:52,279] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-10-09 10:08:52,279] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-10-09 10:08:52,279] torch.distributed.run: [WARNING] *****************************************\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-10-09 10:09:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:03 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-09 10:09:03 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 2\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 1000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.2\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "      exp_dir: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 32\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/exp1/model/llama3_1_8b_instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:09:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:03 exp_manager:396] ExpManager schema\n",
      "[NeMo I 2024-10-09 10:09:03 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-09 10:09:03 exp_manager:830] exp_manager received explicit_log_dir: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen and at least one of exp_dir: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-10-09 10:09:03 exp_manager:835] Exp_manager is logging to /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen, but it already exists.\n",
      "[NeMo W 2024-10-09 10:09:03 exp_manager:757] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:03 exp_manager:455] Experiments will be logged at /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen\n",
      "[NeMo I 2024-10-09 10:09:03 exp_manager:983] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:09:03 exp_manager:1111] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:42 megatron_init:269] Rank 0 has data parallel group : [0, 1]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0, 1]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:280] All data parallel group ranks with context parallel combined: [[0, 1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:294] All context parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:303] All model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:316] All tensor model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:355] All pipeline model parallel group ranks: [[0], [1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:357] All embedding group ranks: [[0], [1]]\n",
      "[NeMo I 2024-10-09 10:09:42 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-10-09 10:09:42 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:42 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:09:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:09:42 megatron_base_model:595] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 10:09:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[rank1]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-10-09 10:11:01 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /root/exp1/model/llama3_1_8b_instruct.nemo.\n",
      "[NeMo I 2024-10-09 10:11:01 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-10-09 10:11:01 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2024-10-09 10:11:04 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:11:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:04 megatron_gpt_sft_model:801] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:495] Building indexing for fn = /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:507] Saving idx file = /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl.idx.npy\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:509] Saving metadata file = /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl.idx.info\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.068018\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.048851\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:249] Loading /root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001096\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-09 10:11:04 megatron_gpt_sft_model:805] Length of val dataset: 2434\n",
      "[NeMo I 2024-10-09 10:11:04 megatron_gpt_sft_model:812] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:495] Building indexing for fn = /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:507] Saving idx file = /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl.idx.npy\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:509] Saving metadata file = /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl.idx.info\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.075523\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:525] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.133688\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:249] Loading /root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000959\n",
      "[NeMo I 2024-10-09 10:11:04 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:11:04 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-10-09 10:11:06 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.13 (sec)\n",
      "[NeMo I 2024-10-09 10:11:06 megatron_gpt_sft_model:814] Length of train dataset: 32160\n",
      "[NeMo I 2024-10-09 10:11:06 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-09 10:11:06 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2024-10-09 10:11:06 megatron_base_model:1223] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-09 10:11:06 nlp_adapter_mixins:324] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | train\n",
      "    ------------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2024-10-09 10:11:06 modelPT:786] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-10-09 10:11:06 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f130e108460>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n",
      "[NeMo I 2024-10-09 10:11:06 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f130e11a410>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 8.0 B  | train\n",
      "------------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "[NeMo W 2024-10-09 10:11:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=126` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-10-09 10:11:06 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:04<00:00,  0.42it/s][NeMo I 2024-10-09 10:11:11 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 10:11:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=126` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-09 10:11:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|██        | 201/1000 [06:52<27:20, reduced_train_loss=1.950, global_step=200.0, consumed_samples=6432.0, train_step_timing in s=2.180]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-09 10:18:04 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏         | 1/77 [00:01<01:22,  0.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 2/77 [00:02<01:21,  0.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 3/77 [00:03<01:20,  0.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▌         | 4/77 [00:05<01:38,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 5/77 [00:06<01:33,  0.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 6/77 [00:09<01:48,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 7/77 [00:10<01:47,  0.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█         | 8/77 [00:12<01:43,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 9/77 [00:13<01:39,  0.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|█▎        | 10/77 [00:14<01:35,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 11/77 [00:15<01:34,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 12/77 [00:16<01:31,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 13/77 [00:18<01:29,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 14/77 [00:19<01:26,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 15/77 [00:20<01:25,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 16/77 [00:21<01:23,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 17/77 [00:22<01:20,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|██▎       | 18/77 [00:24<01:19,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▍       | 19/77 [00:27<01:23,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|██▌       | 20/77 [00:28<01:20,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 21/77 [00:29<01:19,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 22/77 [00:30<01:16,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|██▉       | 23/77 [00:31<01:14,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███       | 24/77 [00:32<01:12,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 25/77 [00:34<01:10,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|███▍      | 26/77 [00:35<01:09,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███▌      | 27/77 [00:36<01:08,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 28/77 [00:39<01:09,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 29/77 [00:40<01:07,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 30/77 [00:42<01:05,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 31/77 [00:43<01:04,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 32/77 [00:44<01:02,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 33/77 [00:45<01:00,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 34/77 [00:46<00:58,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 35/77 [00:48<00:57,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████▋     | 36/77 [00:49<00:56,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 37/77 [00:51<00:55,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████▉     | 38/77 [00:52<00:53,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████     | 39/77 [00:53<00:51,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 40/77 [00:54<00:50,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████▎    | 41/77 [00:55<00:48,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 42/77 [00:56<00:47,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 43/77 [00:58<00:46,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 44/77 [00:59<00:44,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 45/77 [01:00<00:42,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████▉    | 46/77 [01:01<00:41,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 47/77 [01:03<00:40,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 48/77 [01:05<00:39,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 49/77 [01:06<00:38,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████▍   | 50/77 [01:07<00:36,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████▌   | 51/77 [01:09<00:35,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 52/77 [01:10<00:33,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 53/77 [01:11<00:32,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████   | 54/77 [01:13<00:31,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 55/77 [01:14<00:29,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 56/77 [01:15<00:28,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████▍  | 57/77 [01:17<00:27,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 58/77 [01:18<00:25,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 59/77 [01:19<00:24,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 60/77 [01:20<00:22,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 61/77 [01:22<00:21,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████  | 62/77 [01:23<00:20,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 63/77 [01:25<00:18,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 64/77 [01:26<00:17,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 65/77 [01:27<00:16,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 66/77 [01:29<00:14,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|████████▋ | 67/77 [01:30<00:13,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 68/77 [01:31<00:12,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████▉ | 69/77 [01:32<00:10,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 70/77 [01:33<00:09,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 71/77 [01:35<00:08,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▎| 72/77 [01:36<00:06,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████▍| 73/77 [01:37<00:05,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 74/77 [01:39<00:04,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 75/77 [01:40<00:02,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████▊| 76/77 [01:41<00:01,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 77/77 [01:42<00:00,  0.75it/s]\u001b[A[NeMo I 2024-10-09 10:19:46 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved. New best score: 1.666\n",
      "[rank: 1] Metric val_loss improved. New best score: 1.666\n",
      "Epoch 0, global step 201: 'validation_loss' reached 1.66580 (best 1.66580), saving model to '/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.666-step=201-consumed_samples=6432.0.ckpt' as top 1\n",
      "[NeMo W 2024-10-09 10:19:46 nlp_overrides:609] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|████      | 402/1000 [15:34<23:10, reduced_train_loss=1.540, global_step=401.0, consumed_samples=12864.0, train_step_timing in s=2.460, val_loss=1.670]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-09 10:26:46 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏         | 1/77 [00:01<01:19,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 2/77 [00:02<01:17,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 3/77 [00:03<01:17,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▌         | 4/77 [00:05<01:35,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 5/77 [00:06<01:31,  0.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 6/77 [00:08<01:46,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 7/77 [00:10<01:44,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█         | 8/77 [00:11<01:41,  0.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 9/77 [00:12<01:36,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|█▎        | 10/77 [00:13<01:32,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 11/77 [00:15<01:31,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 12/77 [00:16<01:28,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 13/77 [00:17<01:26,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 14/77 [00:18<01:24,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 15/77 [00:20<01:23,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 16/77 [00:21<01:21,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 17/77 [00:22<01:19,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|██▎       | 18/77 [00:23<01:18,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▍       | 19/77 [00:26<01:21,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|██▌       | 20/77 [00:27<01:19,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 21/77 [00:29<01:17,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 22/77 [00:30<01:15,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|██▉       | 23/77 [00:31<01:13,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███       | 24/77 [00:32<01:11,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 25/77 [00:33<01:09,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|███▍      | 26/77 [00:34<01:08,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███▌      | 27/77 [00:36<01:07,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 28/77 [00:39<01:08,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 29/77 [00:40<01:07,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 30/77 [00:41<01:05,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 31/77 [00:42<01:03,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 32/77 [00:43<01:01,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 33/77 [00:44<00:59,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 34/77 [00:46<00:58,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 35/77 [00:47<00:57,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████▋     | 36/77 [00:49<00:56,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 37/77 [00:50<00:54,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████▉     | 38/77 [00:51<00:53,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████     | 39/77 [00:52<00:51,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 40/77 [00:53<00:49,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████▎    | 41/77 [00:54<00:48,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 42/77 [00:56<00:46,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 43/77 [00:57<00:45,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 44/77 [00:58<00:44,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 45/77 [00:59<00:42,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████▉    | 46/77 [01:01<00:41,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 47/77 [01:03<00:40,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 48/77 [01:04<00:39,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 49/77 [01:06<00:37,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████▍   | 50/77 [01:07<00:36,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████▌   | 51/77 [01:08<00:34,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 52/77 [01:09<00:33,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 53/77 [01:11<00:32,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████   | 54/77 [01:12<00:31,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 55/77 [01:13<00:29,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 56/77 [01:15<00:28,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████▍  | 57/77 [01:16<00:26,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 58/77 [01:18<00:25,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 59/77 [01:19<00:24,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 60/77 [01:20<00:22,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 61/77 [01:21<00:21,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████  | 62/77 [01:22<00:20,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 63/77 [01:24<00:18,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 64/77 [01:26<00:17,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 65/77 [01:27<00:16,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 66/77 [01:28<00:14,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|████████▋ | 67/77 [01:29<00:13,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 68/77 [01:30<00:12,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████▉ | 69/77 [01:31<00:10,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 70/77 [01:32<00:09,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 71/77 [01:34<00:07,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▎| 72/77 [01:35<00:06,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████▍| 73/77 [01:37<00:05,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 74/77 [01:38<00:03,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 75/77 [01:39<00:02,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████▊| 76/77 [01:40<00:01,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 77/77 [01:42<00:00,  0.75it/s]\u001b[A[NeMo I 2024-10-09 10:28:28 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.013 >= min_delta = 0.001. New best score: 1.653\n",
      "[rank: 1] Metric val_loss improved by 0.013 >= min_delta = 0.001. New best score: 1.653\n",
      "Epoch 0, global step 402: 'validation_loss' reached 1.65309 (best 1.65309), saving model to '/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.653-step=402-consumed_samples=12864.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|████      | 402/1000 [17:16<25:41, reduced_train_loss=1.540, global_step=401.0, consumed_samples=12864.0, train_step_timing in s=2.460, val_loss=1.650][NeMo I 2024-10-09 10:28:28 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.666-step=201-consumed_samples=6432.0.ckpt\n",
      "[NeMo I 2024-10-09 10:28:28 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.666-step=201-consumed_samples=6432.0-last.ckpt\n",
      "Epoch 0: :  60%|██████    | 603/1000 [24:21<16:02, reduced_train_loss=1.540, global_step=602.0, consumed_samples=19296.0, train_step_timing in s=2.060, val_loss=1.650]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-09 10:35:33 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏         | 1/77 [00:01<01:20,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 2/77 [00:02<01:19,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 3/77 [00:03<01:18,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▌         | 4/77 [00:05<01:36,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 5/77 [00:06<01:31,  0.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 6/77 [00:09<01:46,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 7/77 [00:10<01:44,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█         | 8/77 [00:11<01:41,  0.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 9/77 [00:12<01:37,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|█▎        | 10/77 [00:13<01:33,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 11/77 [00:15<01:32,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 12/77 [00:16<01:29,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 13/77 [00:17<01:27,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 14/77 [00:18<01:24,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 15/77 [00:20<01:23,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 16/77 [00:21<01:21,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 17/77 [00:22<01:19,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|██▎       | 18/77 [00:23<01:18,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▍       | 19/77 [00:26<01:21,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|██▌       | 20/77 [00:27<01:19,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 21/77 [00:29<01:17,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 22/77 [00:30<01:15,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|██▉       | 23/77 [00:31<01:13,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███       | 24/77 [00:32<01:11,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 25/77 [00:33<01:09,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|███▍      | 26/77 [00:34<01:08,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███▌      | 27/77 [00:36<01:07,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 28/77 [00:39<01:08,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 29/77 [00:40<01:06,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 30/77 [00:41<01:04,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 31/77 [00:42<01:02,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 32/77 [00:43<01:01,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 33/77 [00:44<00:59,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 34/77 [00:45<00:57,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 35/77 [00:47<00:56,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████▋     | 36/77 [00:48<00:55,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 37/77 [00:50<00:54,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████▉     | 38/77 [00:51<00:52,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████     | 39/77 [00:52<00:51,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 40/77 [00:53<00:49,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████▎    | 41/77 [00:54<00:47,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 42/77 [00:55<00:46,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 43/77 [00:57<00:45,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 44/77 [00:58<00:43,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 45/77 [00:59<00:42,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████▉    | 46/77 [01:00<00:40,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 47/77 [01:02<00:40,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 48/77 [01:04<00:38,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 49/77 [01:05<00:37,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████▍   | 50/77 [01:07<00:36,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████▌   | 51/77 [01:08<00:34,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 52/77 [01:09<00:33,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 53/77 [01:11<00:32,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████   | 54/77 [01:12<00:30,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 55/77 [01:13<00:29,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 56/77 [01:14<00:28,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████▍  | 57/77 [01:16<00:26,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 58/77 [01:17<00:25,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 59/77 [01:19<00:24,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 60/77 [01:20<00:22,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 61/77 [01:21<00:21,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████  | 62/77 [01:22<00:19,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 63/77 [01:24<00:18,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 64/77 [01:25<00:17,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 65/77 [01:27<00:16,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 66/77 [01:28<00:14,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|████████▋ | 67/77 [01:29<00:13,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 68/77 [01:30<00:11,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████▉ | 69/77 [01:31<00:10,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 70/77 [01:32<00:09,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 71/77 [01:34<00:07,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▎| 72/77 [01:35<00:06,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████▍| 73/77 [01:36<00:05,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 74/77 [01:38<00:03,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 75/77 [01:39<00:02,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████▊| 76/77 [01:40<00:01,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 77/77 [01:41<00:00,  0.76it/s]\u001b[A[NeMo I 2024-10-09 10:37:14 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 1.650\n",
      "[rank: 1] Metric val_loss improved by 0.003 >= min_delta = 0.001. New best score: 1.650\n",
      "Epoch 0, global step 603: 'validation_loss' reached 1.65015 (best 1.65015), saving model to '/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.650-step=603-consumed_samples=19296.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  60%|██████    | 603/1000 [26:03<17:09, reduced_train_loss=1.540, global_step=602.0, consumed_samples=19296.0, train_step_timing in s=2.060, val_loss=1.650][NeMo I 2024-10-09 10:37:15 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.653-step=402-consumed_samples=12864.0.ckpt\n",
      "[NeMo I 2024-10-09 10:37:15 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.653-step=402-consumed_samples=12864.0-last.ckpt\n",
      "Epoch 0: :  80%|████████  | 804/1000 [33:03<08:03, reduced_train_loss=1.380, global_step=803.0, consumed_samples=25728.0, train_step_timing in s=2.280, val_loss=1.650]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-09 10:44:14 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏         | 1/77 [00:01<01:18,  0.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 2/77 [00:02<01:17,  0.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 3/77 [00:03<01:17,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▌         | 4/77 [00:05<01:35,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▋         | 5/77 [00:06<01:31,  0.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|▊         | 6/77 [00:08<01:45,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 7/77 [00:10<01:44,  0.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█         | 8/77 [00:11<01:41,  0.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 9/77 [00:12<01:36,  0.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|█▎        | 10/77 [00:13<01:32,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|█▍        | 11/77 [00:15<01:31,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 12/77 [00:16<01:28,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 13/77 [00:17<01:26,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 14/77 [00:18<01:24,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|█▉        | 15/77 [00:20<01:23,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 16/77 [00:21<01:21,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 17/77 [00:22<01:18,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|██▎       | 18/77 [00:23<01:17,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|██▍       | 19/77 [00:26<01:21,  0.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|██▌       | 20/77 [00:27<01:18,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 21/77 [00:29<01:17,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▊       | 22/77 [00:30<01:15,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|██▉       | 23/77 [00:31<01:13,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███       | 24/77 [00:32<01:11,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|███▏      | 25/77 [00:33<01:09,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|███▍      | 26/77 [00:34<01:07,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|███▌      | 27/77 [00:36<01:06,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 28/77 [00:39<01:08,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 29/77 [00:40<01:06,  0.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 30/77 [00:41<01:04,  0.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 31/77 [00:42<01:02,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 32/77 [00:43<01:00,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████▎     | 33/77 [00:44<00:59,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 34/77 [00:45<00:57,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 35/77 [00:47<00:56,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████▋     | 36/77 [00:48<00:55,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 37/77 [00:50<00:54,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████▉     | 38/77 [00:51<00:52,  0.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████     | 39/77 [00:52<00:50,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 40/77 [00:53<00:49,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████▎    | 41/77 [00:54<00:47,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 42/77 [00:55<00:46,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 43/77 [00:57<00:45,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████▋    | 44/77 [00:58<00:43,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 45/77 [00:59<00:42,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████▉    | 46/77 [01:00<00:40,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 47/77 [01:02<00:39,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 48/77 [01:04<00:38,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 49/77 [01:05<00:37,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████▍   | 50/77 [01:06<00:36,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████▌   | 51/77 [01:07<00:34,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████▊   | 52/77 [01:09<00:33,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 53/77 [01:10<00:32,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████   | 54/77 [01:12<00:30,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████▏  | 55/77 [01:13<00:29,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 56/77 [01:14<00:27,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████▍  | 57/77 [01:16<00:26,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 58/77 [01:17<00:25,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|███████▋  | 59/77 [01:18<00:24,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 60/77 [01:19<00:22,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 61/77 [01:20<00:21,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████  | 62/77 [01:22<00:19,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 63/77 [01:23<00:18,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 64/77 [01:25<00:17,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 65/77 [01:26<00:15,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|████████▌ | 66/77 [01:27<00:14,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|████████▋ | 67/77 [01:28<00:13,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 68/77 [01:30<00:11,  0.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████▉ | 69/77 [01:31<00:10,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 70/77 [01:32<00:09,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|█████████▏| 71/77 [01:33<00:07,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▎| 72/77 [01:34<00:06,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████▍| 73/77 [01:36<00:05,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 74/77 [01:37<00:03,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 75/77 [01:38<00:02,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████▊| 76/77 [01:40<00:01,  0.76it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 77/77 [01:41<00:00,  0.76it/s]\u001b[A[NeMo I 2024-10-09 10:45:56 num_microbatches_calculator:119] setting number of micro-batches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 804: 'validation_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  80%|████████  | 804/1000 [34:44<08:28, reduced_train_loss=1.380, global_step=803.0, consumed_samples=25728.0, train_step_timing in s=2.280, val_loss=1.650][NeMo I 2024-10-09 10:45:56 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.650-step=603-consumed_samples=19296.0-last.ckpt\n",
      "Epoch 0: :  98%|█████████▊| 980/1000 [40:47<00:49, reduced_train_loss=1.660, global_step=979.0, consumed_samples=31360.0, train_step_timing in s=2.040, val_loss=1.650]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 1000/1000 [41:29<00:00, reduced_train_loss=1.590, global_step=999.0, consumed_samples=3.2e+4, train_step_timing in s=2.040, val_loss=1.650]\n",
      "[NeMo I 2024-10-09 10:52:41 nlp_overrides:593] Removing checkpoint: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.653-step=804-consumed_samples=25728.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.650-step=603-consumed_samples=19296.0.ckpt\n",
      "Restored all states from the checkpoint at /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.650-step=603-consumed_samples=19296.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "MODEL=\"/root/exp1/model/llama3_1_8b_instruct.nemo\" #FIXME\n",
    "\n",
    "TRAIN_DS=\"[/root/exp1/data/curated/final/law-qa-train_preprocessed.jsonl]\" #FIXME\n",
    "VALID_DS=\"[/root/exp1/data/curated/final/law-qa-val_preprocessed.jsonl]\"   #FIXME\n",
    "TEST_DS=\"[/root/exp1/data/curated/final/law-qa-test_preprocessed.jsonl]\"   #FIXME\n",
    "TEST_NAMES=\"[law]\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "OUTPUT_DIR=\"/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen\"\n",
    "rm -r $OUTPUT_DIR\n",
    "\n",
    "torchrun --nproc_per_node=2 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${OUTPUT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "    trainer.devices=2 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16-mixed \\\n",
    "    trainer.val_check_interval=0.2 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=32 \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b37c2-26bb-4f07-b635-1cdfb4d9136f",
   "metadata": {},
   "source": [
    "This will create a LoRA adapter - a file named `megatron_gpt_peft_lora_tuning.nemo` in `{YOUR_WORKING_DIR}/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/`. We'll use this later.\n",
    "\n",
    "To further configure the run above -\n",
    "\n",
    "* **A different PEFT technique**: The `peft.peft_scheme` parameter determines the technique being used. In this case, we did LoRA, but NeMo Framework supports other techniques as well - such as P-tuning, Adapters, and IA3. For more information, refer to the [PEFT support matrix](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/nemo_megatron/peft/landing_page.html). For example, for P-tuning, simply set \n",
    "\n",
    "```bash\n",
    "model.peft.peft_scheme=\"ptuning\" # instead of \"lora\"\n",
    "```\n",
    "You can override many such configurations (such as `learning rate`, `adapter dim`, and more) while running the script. A full set of possible configurations is available in [NeMo Framework Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/conf/megatron_gpt_finetuning_config.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709a1d3-333c-4fd2-a1d1-e4f24596c4a0",
   "metadata": {},
   "source": [
    "### Step 3: Inference with NeMo Framework\n",
    "\n",
    "Running text generation within the framework is also possible with running a Python script. Note that is more for testing and validation, not a full-fledged  deployment solution like NVIDIA NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30db1cc0-4ee7-4085-bf54-47535c65c984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307500\n",
      "-rw-r--r-- 1 root root 146928238 Oct  9 10:37 'megatron_gpt_peft_lora_tuning--validation_loss=1.650-step=603-consumed_samples=19296.0.ckpt'\n",
      "-rw-r--r-- 1 root root 146928238 Oct  9 10:52 'megatron_gpt_peft_lora_tuning--validation_loss=1.653-step=1000-consumed_samples=32000.0-last.ckpt'\n",
      "-rw-r--r-- 1 root root  21012480 Oct  9 10:52  megatron_gpt_peft_lora_tuning.nemo\n"
     ]
    }
   ],
   "source": [
    "# Check that the LORA model file exists\n",
    "!ls -l {YOUR_WORKING_DIR}/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c7656-177d-4af6-ad14-8b0bc769264b",
   "metadata": {},
   "source": [
    "In the code snippet below, the following configurations are worth noting - \n",
    "\n",
    "1. `model.restore_from_path` to the path for the Meta-Llama-3.1-8B-Instruct.nemo file.\n",
    "2. `model.peft.restore_from_path` to the path for the PEFT checkpoint that was created in the fine-tuning run in the last step.\n",
    "3. `model.test_ds.file_names` to the path of the preprocessed test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f081844f-f173-41a0-ac39-cd84755a9e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a smaller test subset for a quick eval demonstration.\n",
    "\n",
    "!head -n 128 {DATA_DIR}/law-qa-test_preprocessed.jsonl > {DATA_DIR}/law-qa-test_preprocessed-n128.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06b9d0e7-8c96-4f34-a30d-a0822a9a1887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/exp1/data/curated/final'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8189af4-676d-49c7-ab42-6e025ab02cad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/exp1'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_WORKING_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a37ec5-8fc7-4499-8248-1d825badbf95",
   "metadata": {},
   "source": [
    "If you have made any changes in model or experiment paths, please ensure they are configured correctly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2825989-910a-4a40-a13f-fea58b6cf07b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-10-09 14:26:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:26:15 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-09 14:26:15 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/exp1/model/llama3_1_8b_instruct.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: /root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - /root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl\n",
      "          names:\n",
      "          - law\n",
      "          global_batch_size: 32\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: law_titlegen_lora\n",
      "          truncation_field: null\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          tokens_to_generate: 25\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:26:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:26:41 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-09 14:26:41 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "setting number of micro-batches to constant 1\n",
      "[NeMo I 2024-10-09 14:26:41 tokenizer_utils:183] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:26:42 megatron_base_model:595] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-09 14:26:42 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distributed checkpoint with TensorStoreLoadShardedStrategy\n",
      "[NeMo I 2024-10-09 14:27:57 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /root/exp1/model/llama3_1_8b_instruct.nemo.\n",
      "[NeMo I 2024-10-09 14:27:58 nlp_adapter_mixins:240] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "[NeMo I 2024-10-09 14:28:00 nlp_adapter_mixins:245] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "[NeMo I 2024-10-09 14:28:03 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:28:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-09 14:28:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:28:03 megatron_gpt_sft_model:793] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-10-09 14:28:03 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-09 14:28:03 text_memmap_dataset:525] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:28:06 text_memmap_dataset:495] Building indexing for fn = /root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl\n",
      "[NeMo I 2024-10-09 14:28:06 text_memmap_dataset:507] Saving idx file = /root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl.idx.npy\n",
      "[NeMo I 2024-10-09 14:28:06 text_memmap_dataset:509] Saving metadata file = /root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl.idx.info\n",
      "[NeMo I 2024-10-09 14:28:06 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:02.992823\n",
      "[NeMo I 2024-10-09 14:28:06 text_memmap_dataset:525] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-09 14:28:09 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:03.054275\n",
      "[NeMo I 2024-10-09 14:28:09 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-09 14:28:09 text_memmap_dataset:249] Loading /root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl\n",
      "[NeMo I 2024-10-09 14:28:09 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001248\n",
      "[NeMo I 2024-10-09 14:28:09 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-09 14:28:09 megatron_gpt_sft_model:796] Length of test dataset: 128\n",
      "[NeMo I 2024-10-09 14:28:09 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2024-10-09 14:28:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=254` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-09 14:28:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of micro-batches to constant 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:28:21 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:484: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2024-10-09 14:28:22 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:492: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 32\n",
      "Testing DataLoader 0:  25%|██▌       | 1/4 [01:00<03:01,  0.02it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 32\n",
      "Testing DataLoader 0:  50%|█████     | 2/4 [02:16<02:16,  0.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 32\n",
      "Testing DataLoader 0:  75%|███████▌  | 3/4 [03:29<01:09,  0.01it/s]setting number of micro-batches to constant 1\n",
      "setting number of micro-batches to constant 32\n",
      "Testing DataLoader 0: 100%|██████████| 4/4 [04:25<00:00,  0.02it/s][NeMo I 2024-10-09 14:32:35 megatron_gpt_sft_model:551] Total deduplicated inference data size: 128 to 128\n",
      "[NeMo I 2024-10-09 14:32:35 megatron_gpt_sft_model:702] Predictions saved to law_titlegen_lora_test_law_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:32:35 megatron_gpt_sft_model:642] No training data found, reconfiguring microbatches based on validation batch sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of micro-batches to constant 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-09 14:32:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-09 14:32:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss_law', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-09 14:32:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 4/4 [04:25<00:00,  0.02it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5235300064086914    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m      test_loss_law      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5235300064086914    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5235300064086914    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL=\"/root/exp1/model/llama3_1_8b_instruct.nemo\"\n",
    "\n",
    "TEST_DS=\"[/root/exp1/data/curated/final/law-qa-test_preprocessed-n128.jsonl]\" # Smaller test split\n",
    "# TEST_DS=\"[./curated-data/law-qa-test_preprocessed.jsonl]\" # Full test set\n",
    "TEST_NAMES=\"[law]\"\n",
    "\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# This is where your LoRA checkpoint was saved\n",
    "PATH_TO_TRAINED_MODEL=\"/root/exp1/results/Meta-llama3.1-8B-Instruct-titlegen/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "\n",
    "# The generation run will save the generated outputs over the test dataset in a file prefixed like so\n",
    "OUTPUT_PREFIX=\"law_titlegen_lora\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${MODEL} \\\n",
    "    model.peft.restore_from_path=${PATH_TO_TRAINED_MODEL} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    model.data.test_ds.file_names=${TEST_DS} \\\n",
    "    model.data.test_ds.names=${TEST_NAMES} \\\n",
    "    model.data.test_ds.global_batch_size=32 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=25 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    inference.greedy=True  \\\n",
    "    model.data.test_ds.output_file_path_prefix=${OUTPUT_PREFIX} \\\n",
    "    model.data.test_ds.write_predictions_to_file=True \\\n",
    "    model.data.test_ds.truncation_field=\"null\" \\\n",
    "    model.data.test_ds.add_bos=False \\\n",
    "    model.data.test_ds.add_eos=True \\\n",
    "    model.data.test_ds.add_sep=False \\\n",
    "    model.data.test_ds.label_key=\"output\" \\\n",
    "    model.data.test_ds.prompt_template=\"\\{input\\}\\ \\{output\\}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67568370-0486-44b4-8fb9-64cb266a2500",
   "metadata": {},
   "source": [
    "### Step 4: Check the model accuracy\n",
    "\n",
    "Now that the results are in, let's read the results and calculate the accuracy on the question title generation task.\n",
    "Let's take a look at one of the predictions in the generated output file. The `pred` key indicates what was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f2fc890-5b66-4986-b084-6910ff420003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more. \\nQUESTION: In order to be sued in a particular jurisdiction, say New York, a company must have a minimal business presence in the jurisdiction. What constitutes such a presence? Suppose the company engaged a New York-based Plaintiff, and its representatives signed the contract with the Plaintiff in New York City. Does this satisfy the minimum presence rule? Suppose, instead, the plaintiff and contract signing were in New Jersey, but the company hired a law firm with offices in New York City. Does this qualify? \\nTITLE:\", \"pred\": \" What constitutes a minimal business presence in a jurisdiction?\", \"label\": \" What constitutes \\\"doing business in a jurisdiction?\\\"\"}\n"
     ]
    }
   ],
   "source": [
    "# Take a look at predictions\n",
    "!head -n1  law_titlegen_lora_test_law_inputs_preds_labels.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d6079-315e-490e-9b74-e41cb9d96cbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "For evaluating this task, we will use [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)).  It measures overlap of ngrams, and a higher score is better. While it's not perfect and it misses capturing the semantics of the prediction, it is a popular metric in academia and industry for evaluating such systems. \n",
    "\n",
    "The following method uses the `rouge_score` library to implement scoring. It will report `ROUGE_{1/2/L/Lsum}` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1dd71ca0-5154-4a6b-adbf-a8c40a565d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_rouge(input_file: str) -> dict:\n",
    "    ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(ROUGE_KEYS, use_stemmer=True)\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    lines = [json.loads(line) for line in open(input_file)]\n",
    "    num_response_words = []\n",
    "    num_ref_words = []\n",
    "    for idx, line in enumerate(lines):\n",
    "        prompt = line['input']\n",
    "        response = line['pred']\n",
    "        answer = line['label']\n",
    "        scores = scorer.score(response, answer)\n",
    "        aggregator.add_scores(scores)\n",
    "        num_response_words.append(len(response.split()))\n",
    "        num_ref_words.append(len(answer.split()))\n",
    "\n",
    "    result = aggregator.aggregate()\n",
    "    rouge_scores = {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
    "    print(rouge_scores)\n",
    "    print(f\"Average and stddev of response length: {np.mean(num_response_words):.2f}, {np.std(num_response_words):.2f}\")\n",
    "    print(f\"Average and stddev of ref length: {np.mean(num_ref_words):.2f}, {np.std(num_ref_words):.2f}\")\n",
    "\n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21be1472-1a63-41c1-86b2-fd4404cf9928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 39.972, 'rouge2': 19.6546, 'rougeL': 35.9545, 'rougeLsum': 35.8495}\n",
      "Average and stddev of response length: 10.40, 4.38\n",
      "Average and stddev of ref length: 11.26, 4.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 39.972, 'rouge2': 19.6546, 'rougeL': 35.9545, 'rougeLsum': 35.8495}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rouge(\"./law_titlegen_lora_test_law_inputs_preds_labels.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fd720-d9bd-4ccb-8582-2b62e4e5e31e",
   "metadata": {},
   "source": [
    "For the Llama-3.1-8B-Instruct model, you should see accuracy comparable to the below:\n",
    "```\n",
    "{'rouge1': 39.2082, 'rouge2': 18.8573, 'rougeL': 35.4098, 'rougeLsum': 35.3906}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
